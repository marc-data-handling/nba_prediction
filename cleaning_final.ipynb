{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0dc4d4-8477-4f1d-9b90-f78f50edd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72863a3-49ac-4bff-9b0d-9f7cabadea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import the uncleanded but merged dataset\n",
    "df=pd.read_csv(\"data/data_uncleaned.csv\")\n",
    "#We need all informations in regard to the amount of games played\n",
    "df[\"W_pct\"] = (df[\"W\"] / df[\"G\"])\n",
    "df[\"PW_pct\"] = (df[\"PW\"] / df[\"G\"])\n",
    "#We drop the columns that have no relevant information about the performance of a team\n",
    "#Furthermore, we dropped all variables that store absolute values instead of values per game\n",
    "#because they do not allow to compare seasons with a different amount of games\n",
    "#Furthermore, we dropped Attend. and Attend./G because they contain missing values\n",
    "df = df.drop(['Unnamed: 27', 'Unnamed: 22', 'Unnamed: 17', 'Season', '1997_98', 'Arena', 'Attend.','Attend./G',\n",
    "              'FGA', 'FG', '3P', '3PA', '2P', '2PA', 'FTA', 'FT', 'L', 'PL', 'SRS', 'ORtg', 'DRtg', 'NRtg', 'Pace',\n",
    "              'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'eFG%.1', 'TOV%.1', 'DRB%', 'FT/FGA.1', 'Season_x', 'Season_y',\n",
    "              'Rk_y', 'Team', 'W', 'PW', 'G'], axis=1)\n",
    "#Every team has been ranked in each season between 1 and 30 in every season\n",
    "#If there is no information in this column, there is no important information in the row at all\n",
    "#Therefore, we drop all rows that do contain missing values in the Rk_x axis\n",
    "df=df.dropna(subset=['Rk_x'])\n",
    "#The information in the column RK_x is not relevant and therefore will be dropped\n",
    "df = df.drop(['Rk_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e4d87c-5bf7-49cb-959d-c7544d518723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We save the cleaned data set\n",
    "df.to_csv(\"data/df_cleaned_before_manipulation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ee8648-7be9-44fe-8387-68c52d306bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afterwards we found a mistake in our data while analyzing it\n",
    "#We corrected the mistake manually and checked whether our data set is still good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33a3b72-ca5f-47d6-b055-c2b092f193ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/df_cleaned_manipulated.csv\")\n",
    "#Somehow there is a empty column called Unnamed: 0, which we have to drop\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0e0241-c41f-49a1-b7c3-24ffdc8cf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now safe our final data set as df_cleaned\n",
    "df.to_csv(\"data/df_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1448a5-2c46-4925-abc9-fe42784487c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For our project we need to evaluate how many principal components we need to explain 85% of the variance of the data\n",
    "def find_min_k_pca(X, threshold=0.85):\n",
    "    #Instantiate the PCA object with the number of components being the columns of the input matrix\n",
    "    pca = PCA(n_components=X.shape[1])\n",
    "    #Standardize inputs\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    pca.fit(X)\n",
    "    #Find the index of the first element for which the cumulative variance explained\n",
    "    #is at least the given threshold\n",
    "    k = np.argmax(pca.explained_variance_ratio_.cumsum() >= threshold)\n",
    "    #The function returns the number of principal components we need to explain 85% of the variance\n",
    "    return k\n",
    "#We call our function so that it gives us the number of principal components we need to explain 85% of our data\n",
    "find_min_k_pca(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
